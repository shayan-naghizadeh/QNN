{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "622dd8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy, torch, torch.nn as nn, torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import snntorch as snn\n",
    "from snntorch import spikegen\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de36e18",
   "metadata": {},
   "source": [
    "**parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3cfc166",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_in, num_hid, num_out = 784, 256, 10\n",
    "beta, T, bs, epochs, lr = 0.9, 50, 128, 3, 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38397089",
   "metadata": {},
   "source": [
    "**device**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "554678c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1216d2a8ad0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874dbaa8",
   "metadata": {},
   "source": [
    "**arch network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f2d2501",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1  = nn.Linear(num_in , num_hid, bias=False)\n",
    "        self.lif1 = snn.Leaky(beta=beta)\n",
    "        self.fc2  = nn.Linear(num_hid, num_out, bias=False)\n",
    "        self.lif2 = snn.Leaky(beta=beta)\n",
    "    def forward(self, x):\n",
    "        x = x.to(self.fc1.weight.dtype)\n",
    "        mem1 = self.lif1.init_leaky().to(device=x.device, dtype=x.dtype)\n",
    "        mem2 = self.lif2.init_leaky().to(device=x.device, dtype=x.dtype)\n",
    "        out  = []\n",
    "        for t in range(x.size(0)):\n",
    "            spk1, mem1 = self.lif1(self.fc1(x[t]), mem1)\n",
    "            spk1 = spk1.to(dtype=self.fc2.weight.dtype)        #\n",
    "            spk2, mem2 = self.lif2(self.fc2(spk1), mem2)\n",
    "            out.append(spk2)\n",
    "        return torch.stack(out)                                # [T,B,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cdd6624d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = transforms.Compose([transforms.ToTensor()])\n",
    "train_ds = torchvision.datasets.MNIST(\"data\", True , download=True, transform=tr)\n",
    "test_ds  = torchvision.datasets.MNIST(\"data\", False, download=True, transform=tr)\n",
    "train_ld = DataLoader(train_ds, bs, True , num_workers=2, pin_memory=True)\n",
    "test_ld  = DataLoader(test_ds , bs, False, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff8cf7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = SNN().to(device)\n",
    "opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa76cf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01de613b",
   "metadata": {},
   "source": [
    "**train phase**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0468a221",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net,train_ld,epochs):\n",
    "    for ep in range(1, epochs + 1):\n",
    "        net.train()\n",
    "        for imgs, lbls in tqdm(train_ld, desc=f\"train {ep}/{epochs}\"):\n",
    "            imgs = imgs.to(device).view(imgs.size(0), -1) \n",
    "            lbls = lbls.to(device)\n",
    "            spk  = spikegen.rate(imgs, T).to(device)\n",
    "            loss = loss_fn(net(spk).sum(0), lbls)\n",
    "            opt.zero_grad(); loss.backward(); opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb0376ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train 1/3:  27%|██▋       | 126/469 [00:17<00:48,  7.01it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_ld\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[19], line 8\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(net, train_ld, epochs)\u001b[0m\n\u001b[0;32m      6\u001b[0m lbls \u001b[38;5;241m=\u001b[39m lbls\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      7\u001b[0m spk  \u001b[38;5;241m=\u001b[39m spikegen\u001b[38;5;241m.\u001b[39mrate(imgs, T)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m----> 8\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspk\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m0\u001b[39m), lbls)\n\u001b[0;32m      9\u001b[0m opt\u001b[38;5;241m.\u001b[39mzero_grad(); loss\u001b[38;5;241m.\u001b[39mbackward(); opt\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[15], line 14\u001b[0m, in \u001b[0;36mSNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     12\u001b[0m out  \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)):\n\u001b[1;32m---> 14\u001b[0m     spk1, mem1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlif1\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmem1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m     spk1 \u001b[38;5;241m=\u001b[39m spk1\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdtype)        \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     spk2, mem2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlif2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(spk1), mem2)\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\snntorch\\_neurons\\leaky.py:220\u001b[0m, in \u001b[0;36mLeaky.forward\u001b[1;34m(self, input_, mem)\u001b[0m\n\u001b[0;32m    216\u001b[0m     spk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfire_inhibition(\n\u001b[0;32m    217\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmem\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmem\n\u001b[0;32m    218\u001b[0m     )  \u001b[38;5;66;03m# batch_size\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 220\u001b[0m     spk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_delay:\n\u001b[0;32m    223\u001b[0m     do_reset \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    224\u001b[0m         spk \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraded_spikes_factor \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset\n\u001b[0;32m    225\u001b[0m     )  \u001b[38;5;66;03m# avoid double reset\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\snntorch\\_neurons\\neurons.py:81\u001b[0m, in \u001b[0;36mSpikingNeuron.fire\u001b[1;34m(self, mem)\u001b[0m\n\u001b[0;32m     78\u001b[0m     mem \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_quant(mem)\n\u001b[0;32m     80\u001b[0m mem_shift \u001b[38;5;241m=\u001b[39m mem \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthreshold\n\u001b[1;32m---> 81\u001b[0m spk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspike_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmem_shift\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m spk \u001b[38;5;241m=\u001b[39m spk \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraded_spikes_factor\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m spk\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\snntorch\\surrogate.py:211\u001b[0m, in \u001b[0;36matan.<locals>.inner\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(x):\n\u001b[1;32m--> 211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mATan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\function.py:575\u001b[0m, in \u001b[0;36mFunction.apply\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[0;32m    573\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[0;32m    574\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[1;32m--> 575\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    583\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\snntorch\\surrogate.py:190\u001b[0m, in \u001b[0;36mATan.forward\u001b[1;34m(ctx, input_, alpha)\u001b[0m\n\u001b[0;32m    188\u001b[0m ctx\u001b[38;5;241m.\u001b[39msave_for_backward(input_)\n\u001b[0;32m    189\u001b[0m ctx\u001b[38;5;241m.\u001b[39malpha \u001b[38;5;241m=\u001b[39m alpha\n\u001b[1;32m--> 190\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43minput_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(net,train_ld,epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae666a1",
   "metadata": {},
   "source": [
    "**test phase**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f888115",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model,loader):\n",
    "    @torch.no_grad()\n",
    "    def evaluate(model, loader, dtype):\n",
    "        model.eval()\n",
    "        correct = total = 0\n",
    "        for imgs, lbls in loader:\n",
    "            imgs = (imgs.to(device).view(imgs.size(0), -1) ).to(dtype)\n",
    "            spk  = spikegen.rate(imgs, T).to(dtype).to(device)\n",
    "            preds = model(spk).sum(0).argmax(1)\n",
    "            correct += (preds == lbls.to(device)).sum().item()\n",
    "            total   += lbls.size(0)\n",
    "        return 100 * correct / total\n",
    "\n",
    "\n",
    "    acc32 = evaluate(net, test_ld, torch.float32)\n",
    "\n",
    "\n",
    "    net_fp16 = SNN().to(device)\n",
    "    net_fp16.load_state_dict(net.state_dict())\n",
    "    net_fp16.half().eval()\n",
    "    acc16 = evaluate(net_fp16, test_ld, torch.float16)\n",
    "\n",
    "    print(f\"Accuracy FP32 : {acc32:.2f}%\")\n",
    "    print(f\"Accuracy FP16 : {acc16:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "873a3511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy FP32 : 96.33%\n",
      "Accuracy FP16 : 96.27%\n"
     ]
    }
   ],
   "source": [
    "test(net,test_ld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0b9e551",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(),'model_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "14f43613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.load_state_dict(torch.load('model_weights.pth', map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e5b0c70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc2w = net.fc2.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c3ba29e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc2w[9].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591156b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "with open(\"weights.cpp\", \"w\") as cpp_file:\n",
    "    cpp_file.write(\"#include <vector>\\n\\n\")\n",
    "\n",
    "    for name, param in net.state_dict().items():\n",
    "        cpp_name = name.replace(\".\", \"_\")\n",
    "        data = param.cpu().numpy()\n",
    "        shape = data.shape\n",
    "\n",
    "        cpp_file.write(f\"// shape: {shape}\\n\")\n",
    "        cpp_file.write(f\"std::vector<float> {cpp_name} = {{\\n    \")\n",
    "        cpp_file.write(\", \".join(f\"{v:.6f}\" for v in data.flatten()))\n",
    "        cpp_file.write(\"\\n};\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
